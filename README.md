ğŸŒ¦ï¸ Weather Forecast Data Pipeline (Airflow + PostgreSQL + Power BI)
ğŸ“Œ Project Overview

This project is an end-to-end Weather Forecast Data Pipeline built with Apache Airflow, designed to extract, transform, and load (ETL) weather forecast data into a PostgreSQL database, making it ready for analytics and visualization.

The pipeline fetches forecast data from the Visual Crossing Weather API, processes and cleans the dataset using Pandas, loads it into PostgreSQL with UPSERT logic to prevent duplicate records, and finally connects the database to a Power BI dashboard for interactive reporting.

This project was fully containerized using Docker Compose, ensuring portability and easy deployment.

## ğŸš€ Tech Stack

- Python
- Apache Airflow
- PostgreSQL
- Pandas
- psycopg2
- Docker / Docker Compose
- Power BI
- Visual Crossing API

## ğŸ¯ Main Features

- âœ… Automated daily forecast ingestion using **Airflow DAG scheduling**
- âœ… Extraction of weather forecast data via **Visual Crossing API**
- âœ… Data transformation and enrichment using **Pandas**
- âœ… Generation of **raw and processed CSV backups**
- âœ… Load into **PostgreSQL** with automatic table creation
- âœ… **UPSERT** implementation to prevent duplicate data and allow updates
- âœ… **Power BI dashboard** connected to PostgreSQL for visualization

ğŸ—ï¸ Pipeline Architecture

The ETL process is implemented in a Python module called:

ğŸ“Œ pipeline_tempo.py (or Pipeline Tempo)

It contains three main functions:

1) extrair_dados(cidade, dias)

Builds the API URL using the selected city and forecast range.
Fetches data directly from Visual Crossing.
Loads the API response into a Pandas DataFrame.
Saves the raw dataset into a CSV file for backup and debugging.

Output: Raw DataFrame


2) processar_dados(df)

Receives the raw DataFrame.
Creates new derived columns.
Filters only relevant columns for analytics.
Saves the cleaned dataset into another CSV file.

Output: Processed DataFrame


3) carregar_postgre(df, tabela="previsao_tempo")

Connects to PostgreSQL using psycopg2.
Creates the table if it does not exist.
Inserts the records into PostgreSQL.
Uses UPSERT logic (INSERT ... ON CONFLICT DO UPDATE) to prevent duplicates.

Output: None (writes data into PostgreSQL)

â³ Airflow DAG Orchestration

The ETL process is orchestrated using an Apache Airflow DAG.
The DAG imports the functions from the pipeline module and executes them as tasks in the correct order:

Extract Task â†’ calls extrair_dados()
Transform Task â†’ calls processar_dados()
Load Task â†’ calls carregar_postgre()

The DAG is scheduled to run daily.

ğŸ“Œ DAG Name: pipeline_clima
ğŸ“Œ Schedule: @daily

ğŸ³ Dockerized Environment

The entire Airflow environment runs inside Docker containers using Docker Compose.

The project includes:
docker-compose.yml for orchestrating Airflow + PostgreSQL
Dockerfile for installing Python dependencies (requirements.txt)

Mounted volumes for:

DAGs
Logs
Scripts / pipeline code
Generated CSV backups
This ensures the project can be executed consistently on any machine.

## ğŸ“‚ Project Structure

```bash
airflow-tempo-projeto
â”‚
â”œâ”€â”€ PowerBIImages
â”‚   â”œâ”€â”€ page1_powerbiweekforecast.png
â”‚   â””â”€â”€ page2_powerbiweekforecast.png
â”œâ”€â”€ airflow
â”‚   â”œâ”€â”€ dags
â”‚   â”‚   â””â”€â”€ pipeline_clima.py
â”‚   â””â”€â”€ logs    
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ processed
â”‚   â”‚   â””â”€â”€ previsao_tempo_limpo.csv
â”‚   â””â”€â”€ raw
â”‚       â””â”€â”€ previsao_tempo.csv
â”œâ”€â”€ docker
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ sql
â”‚   â””â”€â”€ create_tables.sql
â”œâ”€â”€ src
â”‚   â””â”€â”€ pipeline_tempo.py
â””â”€â”€ README.md 
```


## ğŸ—„ï¸ Table Schema (PostgreSQL)

The pipeline loads data into the following table:

ğŸ“Œ **Table name:** `previsao_tempo`

```sql
CREATE TABLE previsao_tempo (
    name TEXT NOT NULL,
    datetime TIMESTAMP NOT NULL,

    tempmax DOUBLE PRECISION,
    tempmin DOUBLE PRECISION,
    temp DOUBLE PRECISION,
    feelslike DOUBLE PRECISION,
    precip DOUBLE PRECISION,
    humidity DOUBLE PRECISION,
    windspeed DOUBLE PRECISION,

    avg_temp DOUBLE PRECISION,
    temp_range DOUBLE PRECISION,
    day_of_week TEXT,

    collected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    CONSTRAINT pk_previsao_tempo PRIMARY KEY (name, datetime)
);
```

ğŸ“Š Power BI Dashboard

After the ETL process loads the data into PostgreSQL, the database is connected to Power BI for visualization.

The Power BI report includes two main pages:

ğŸ“Œ 1) Tomorrowâ€™s Weather Forecast

This page focuses on:

Tomorrowâ€™s forecast highlights
    Average Temperature
    Humidity
    Wind speed

It also includes a chart about the Temperature Forecast for Next Week

ğŸ“Œ 2) Weather Overview

This page provides:

Full week forecast analysis
    Humidity
    Precipitation
    Temperature forecast for the next week(including Max temp, Avg Temp, Min temp.)

 - The dashboard updates when Power BI refresh is triggered, pulling the latest data directly from PostgreSQL.

## ğŸ“¸ Power BI Dashboard Preview

### Tomorrowâ€™s Weather Forecast
![Tomorrow Forecast](PowerBIImages/powerBI-airflow-tempo-page1.png)

### Weather Overview
![Weekly Overview](PowerBIImages/powerBI-airflow-tempo-page2.png)
